---
title: "Assignment 3"
author: "Sylvia Baeyens"
date: "due 11/5/2021"
output:
  github_document: 
    html_preview: false
  html_document: default
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r packages, echo= FALSE, include= FALSE}
#including necessary libraries
library(tidyverse)
library(tidytext)
library(dplyr)
library(ggplot2)
library(data.table)
library(httr)
```

# APIs

## 1. Using the NCBI API, look for papers that show up under the term “sars-cov-2 trial vaccine.” Look for the data in the pubmed database, and then retrieve the details of the paper as shown in lab 7. How many papers were you able to find?
```{r NCBI API}

website = xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine")
count = xml2::xml_find_first(website, "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")
count = as.character(count)
numCount = stringr::str_extract(count, "[0-9,]+")

```
There are 2339 papers.

## 2. Using the list of pubmed ids you retrieved, download each papers’ details using the query parameter rettype = abstract. If you get more than 250 ids, just keep the first 250.
```{r paper details}

query_ids = GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "sars-cov-2 trial vaccine",
    retmax = 1000
  )
)
ids = httr::content(query_ids)
ids = as.character(ids)
ids = stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
ids = stringr::str_remove_all(ids, "<Id>|</Id>")
ids = ids[1:250] #to keep only first 250
ids2 = I(paste(ids, collapse = ","))

pubs = GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi",
  query = list(
    db =  "pubmed",
    id = ids2,
    retmax = 1000,
    rettype = "abstract"
    )
)

pubs = httr::content(pubs)
pubsText = as.character(pubs)
```

## 3. As we did in lab 7. Create a dataset containing the following: Pubmed ID number, Title of the paper,
Name of the journal where it was published, Publication date, and Abstract of the paper (if any).

```{r publication dataset}

pub_char_list = xml2::xml_children(pubs)
pub_char_list = sapply(pub_char_list, as.character)
# Title of the Paper
title = str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
title = str_remove_all(title, "</?[[:alnum:]- =\"]+>")
# Name of Journal where it was Published
journal = str_extract(pub_char_list, "<Title>[[:print:][:space:]]+</Title>")
journal = str_remove_all(journal, "</?[[:alnum:]- =\"]+>")
journal = str_replace_all(journal, "[[:space:]]+", " ")
# Publication Date
PubDate = str_extract(pub_char_list, "<PubDate>[[:print:][:space:]]+</PubDate>")
PubDate = str_remove_all(PubDate, "</?[[:alnum:]- =\"]+>")
PubDate = str_replace_all(PubDate, "[[:space:]]+", " ")
# Abstract of the paper (if any)
abstracts = str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts = str_remove_all(abstracts, "</?[[:alnum:]- =\"]+>")
abstracts = str_replace_all(abstracts, "[[:space:]]+", " ")


# Create a database
database = data.frame(
  Journal = journal,
  PublicationDate = PubDate,
  PubMedID = ids,
  Title = title,
  Abstract = abstracts
)
knitr::kable(database[1:15, ], caption = "Summary of first Sars-2 Covid Vaccine Academic Papers")
```

# Text Mining

```{r add new datset}
if (!file.exists("pubmed.csv")) {
download.file("https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv",
              destfile = "pubmed.csv", 
              method="libcurl", 
              timeout = 60
              )
}
pubmedData = data.table::fread("pubmed.csv")
```

## Tokenize the abstracts and count the number of each token. Do you see anything interesting? Does removing stop words change what tokens appear as the most frequent? What are the 5 most common tokens for each search term after removing stopwords?

```{r tokenizing abstracts}
pubmedData %>%
  unnest_tokens(token, abstract) %>%
  count(token, sort = TRUE) %>%
  top_n(15, n)%>%
  knitr::kable(caption = "15 Most Frequent Token Words without removing Stop Words")
```
The 10 most frequent tokens are all stop words, with the exception of covid and 19, as is to be expected. As we go further down the list to 11-15, we see more medically relevant words such as patient and cancer.

```{r tokens without stop words}
pubmedData %>%
  unnest_tokens(token, abstract) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  count(token, sort = TRUE) %>%
  top_n(15, n)%>%
  knitr::kable(caption = "15 Most Frequent Token Words with Stop Words Removed")
```
Now all the token words have medical relevance. The 5 most common are covid, 19, patients, cancer, and prostate.

```{r by each search term}
pubmedData %>% 
  unnest_tokens(token, abstract)%>%
  anti_join(stop_words,by = c("token" = "word"))%>%
  group_by(term)%>%
  count(token)%>%
  top_n(5,n)%>%
  knitr::kable(caption="5 Most Frequent Tokens Per Search Term")
```
Here we see that the different search terms were covid, cystic fibrosis, meningitis, preeclampsia, and prostate cancer. This table details the 5 most frequent token for each search term. There is some overlap between search terms.

## Tokenize the abstracts into bigrams. Find the 10 most common bigram and visualize them with ggplot2.

```{r bigrams}
pubmedData %>%
  unnest_ngrams(output = bigram, input = abstract, n = 2) %>%
  separate(bigram, into = c("word1", "word2"), sep = " ") %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word")) %>%
  unite(bigram, c("word1", "word2"), sep = " ") %>%
  count(bigram, sort = TRUE) %>%
  top_n(10)%>%
  ggplot(aes(x = n, y = fct_reorder(bigram, n))) +
  geom_col(fill= "royal blue") +
  labs(title = "10 Most Frequent Bigrams with Stop Words Removed")
```
This plot shows all the most frequent bigrams. They are all medically related.

## Calculate the TF-IDF value for each word-search term combination. (here you want the search term to be the “document”) What are the 5 tokens from each search term with the highest TF-IDF value? How are the results different from the answers you got in question 1?

```{r TF-IDF}
pubmedData %>%
  unnest_tokens(word, abstract) %>%
  count(word, term) %>%
  bind_tf_idf(word, term, n) %>%
  group_by(term)%>%
  top_n(5,tf_idf)%>%
  arrange(desc(tf_idf), .by_group = TRUE)%>%
  knitr::kable(caption="5 Tokens with the highest TF-IDF values for each search term ")
```

The table above shows the 5 Token Words with the highest TF-IDF values for each of the 5 search terms. There is overlap between these words and the result of Q1, where we searched for most frequent words. However, this new list appears to contain words with greater specificity, such as androgen, pachymeningitis, and cftr and fewer general words such as patient or disease. This is because the TF-IDf takes into account the importance of the word to the paper, and words with greater specificity will have greater importance. 

